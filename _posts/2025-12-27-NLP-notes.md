---
layout: post
title: Notes on NLP
---


# N-gram LMs revisited / Sequence Tagging: HMMs

## Language Models (LMs)

A **language model** assigns probabilities to sequences and to next-word predictions:

- Sequence probability: $$P(w_1 w_2 \dots w_n)$$
- Next-word distribution: $$P(w_n \mid w_1 w_2 \dots w_{n-1})$$


### Vocabulary + dataset

- Vocabulary (finite set): $$\mathcal{V} = \{\texttt{the}, \texttt{a}, \texttt{man}, \dots\}$$
- All possible sequences (infinite): $$\mathcal{V}^*$$
- Training corpus: $$\mathcal{D} = \{\mathbf{w}^{(m)}\}_{m=1}^M$$


### Naïve empirical distribution (doesn't generalize)

A naïve approach is the empirical distribution:

$$
P(\mathbf{w}) = \frac{c(\mathbf{w})}{M}
$$

This fails to generalize to valid sequences that never appear in the training set.


### Chain rule decomposition

Using the chain rule:

$$
P(w_1 w_2 \dots w_n) = \prod_{i=1}^{n} P\big(w_i \mid w_1\dots w_{i-1}\big)
$$

Example:

$$
P(\text{I saw a man}) = P(\text{I})\,P(\text{saw}\mid\text{I})\,P(\text{a}\mid\text{I saw})\,P(\text{man}\mid\text{I saw a})
$$


### Markov assumption → n-gram LMs

**Key idea (Markov assumption):** approximate each conditional with only the last $$(N-1)$$ words.

- Unigram: $$P(w_i)$$
- Bigram: $$P(w_i\mid w_{i-1})$$
- Trigram: $$P(w_i\mid w_{i-2}, w_{i-1})$$

Example query:
- $$P(\text{lost} \mid \text{Not all those who wander are})$$
  - unigram: $$P(\text{lost})$$
  - bigram: $$P(\text{lost}\mid\text{are})$$
  - trigram: $$P(\text{lost}\mid\text{wander are})$$


### Bigram sequence probability

With implicit start token $$\langle s \rangle$$:

$$
P(w_1\dots w_n) \approx \prod_{i=1}^{n} P\big(w_i \mid w_{i-1}\big),\quad w_0 := \langle s \rangle
$$


## Learning an n-gram model (MLE / “raw counts”)

For a bigram LM, **MLE** estimates:

$$
\hat P(w_i\mid w_{i-1}) = \frac{c(w_{i-1}, w_i)}{c(w_{i-1})}
$$

You need:
- unigram counts $$c(w)$$
- bigram counts $$c(w_{i-1}, w_i)$$


### Worked toy example

Training corpus:

```
<s> I get what I eat and
I eat what I get </s>
```

From counts, the slide derives:

$$
P(\text{what} \mid \text{get}) = \tfrac{1}{2}
$$

And:

$$
P(\langle s\rangle\;\text{I get what}) = P(\text{I}\mid\langle s\rangle)\,P(\text{get}\mid\text{I})\,P(\text{what}\mid\text{get}) = 1\cdot\frac{2}{4}\cdot\frac{1}{2} = 0.25
$$

Note: the model can assign non-zero probability to some unseen *sentences*, but any unseen **bigram** makes the full sequence probability $$0$$ → motivates smoothing.


## Practical issues when building n-gram LMs

### Types vs. tokens
- **Tokens** = word instances in running text
- **Types** = distinct word forms in the vocabulary


### Counting & normalization choices
- Tokenization decisions (punctuation, contractions)
- Language-dependent tokenization
- Text normalization (e.g., case-folding) depends on downstream task


### Engineering tips
- Compute in **log space** to avoid underflow and to replace products with sums.
- If space is an issue, don’t explicitly store 0-count n-grams; treat missing entries as count 0.


## Generating text by sampling from an LM

For a bigram LM:
1. Sample a bigram starting with $$\langle s\rangle$$; output the second word.
2. Repeat: sample a bigram conditioned on the last generated word.
3. Stop at end token or another stopping criterion.


## Part-of-Speech (POS) tagging

**Goal:** assign a POS tag to each token in a sentence (sequence tagging).

Example:

```
The/DT planet/NN Jupiter/NNP and/CC its/PPS moons/NNS are/VBP in/IN effect/NN a/DT mini-solar/JJ system/NN ./.
```

Why it’s hard:
- Many common words are ambiguous (e.g., *book* noun vs verb)
- Ambiguity is frequent in running text


### Quick notes from the slides

- Penn Treebank tagset has 45 tags (example shown on slides).
- A strong baseline: choose the most frequent tag per word (~92% accuracy).
- SOTA methods reach ~98% accuracy.


## HMMs for sequence tagging (intro)

We want the best tag sequence $$t_1\dots t_n$$ for words $$w_1\dots w_n$$:

$$
\arg\max_{t_1\dots t_n} P(t_1\dots t_n \mid w_1\dots w_n)
$$

Using Bayes (denominator ignored for argmax):

$$
\arg\max_{t} P(w_1\dots w_n \mid t_1\dots t_n)\,P(t_1\dots t_n)
$$

Assumptions:
- **Tag sequence model** (Markov / n-gram): e.g. bigram $$P(t_1\dots t_n) \approx \prod_{i=1}^n P(t_i\mid t_{i-1})$$
- **Emissions** (conditional independence): $$P(w_1\dots w_n\mid t_1\dots t_n) \approx \prod_{i=1}^n P(w_i\mid t_i)$$

So the score factors into **transition** and **emission** probabilities.


### Brute-force decoding is impossible

If sentence length $$m=20$$ and tagset size $$T=15$$, then there are $$T^m = 15^{20}$$ possible tag sequences → too many to enumerate. Next class: efficient decoding (Viterbi).
